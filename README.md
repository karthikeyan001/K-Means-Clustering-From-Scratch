K-Means Clustering From Scratch 

A complete implementation of the K-Means clustering algorithm using only NumPy, without scikit-learnâ€™s KMeans.
This project includes full clustering logic, dataset generation, visualization, Elbow Method, Silhouette Analysis, and a final evaluation report.

Overview

This project focuses on implementing the K-Means unsupervised learning algorithm from scratch, with the purpose of understanding:

How centroids are initialized

How distances are calculated

How data points are reassigned during iterations

How convergence is reached

How to evaluate clustering quality using

Elbow Method (SSE)

Silhouette Score (NumPy custom implementation)

The entire workflow â€” from dataset generation to evaluation and visualization â€” is done in one Python file for simplicity and accuracy.

 Objectives

âœ” Implement K-Means algorithm using NumPy (no ML libraries)
âœ” Generate well-separated synthetic data (Gaussian blobs)
âœ” Determine the optimal number of clusters using:

Elbow Method

Silhouette Analysis
âœ” Visualize final clusters & centroids
âœ” Provide summary, interpretation, and full analysisMethodology
1. Data Generation

We use sklearn.datasets.make_blobs() ONLY for dataset creation.
The dataset includes:

600 samples

2 numeric features

4 true cluster centers

Controlled separation

This produces clean data suitable for visual clustering.

2. K-Means Implementation (Pure NumPy)

The algorithm was implemented manually with the following components:

âœ” Random Centroid Initialization

Random data points chosen as initial cluster centers.

âœ” Distance Calculation

Euclidean distance (L2 norm) computed between every point and centroid.

âœ” Cluster Assignment

Each sample is assigned to its nearest centroid.

âœ” Centroid Update

New centroid = mean of all points assigned to that cluster.

âœ” Convergence

Algorithm stops when centroid movement < tolerance threshold.

This ensures a fully working K-Means algorithm without relying on scikit-learn.

ðŸ“ˆ 3. Evaluation Metrics

To determine the best value of K, two independent methods were applied:

ðŸ”¹ A. Elbow Method (Sum of Squared Errors â€” SSE)

SSE is computed for K = 1 to 10.
The â€œelbow pointâ€ is where reduction in error slows down sharply.

ðŸŸ¢ Optimal K from Elbow Method = 4

ðŸ”¹ B. Silhouette Score (Custom NumPy Implementation)

For each value of K (2 to 10):

a(i) = Mean distance to points in same cluster

b(i) = Mean distance to closest neighboring cluster

Silhouette = (b âˆ’ a) / max(a, b)

Higher silhouette score â†’ better cluster quality.

ðŸŸ¢ Optimal K from Silhouette Score = 4

ðŸ“Œ 4. Final Results

Both evaluation methods independently selected the same value:

Method	Chosen K
Elbow Method	4
Silhouette Score	4

A final K-Means model with K=4 was trained and visualized.
Clusters are clearly separated, and centroids are correctly positioned.
